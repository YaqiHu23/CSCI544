{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2xIxQrL003A",
        "outputId": "50e64217-9b7b-4b3a-8076-529574f8e43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/memsizer\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/memsizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fairseq==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWXfID5Tq4I6",
        "outputId": "fc715e6c-34d1-4f22-c963-ae2d66f747a1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairseq==0.10.0 in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (3.0.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (0.6)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (0.6.2)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (1.23.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (2.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.10.0) (4.66.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.10.0) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.10.0) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.10.0) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.10.0) (4.9.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.10.0) (2.21)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core->fairseq==0.10.0) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core->fairseq==0.10.0) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core->fairseq==0.10.0) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.10.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.10.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.10.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.10.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.10.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.10.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq==0.10.0) (2.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core->fairseq==0.10.0) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq==0.10.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq==0.10.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.memsizer_layer import *"
      ],
      "metadata": {
        "id": "ANeuLjbf2N4g"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse"
      ],
      "metadata": {
        "id": "AX0L-qBz2nxn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "SYuEwuBx2_Zo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, pad_token=\"<pad>\", unk_token='<unk>', eos_token='<eos>', sos_token='<sos>'):\n",
        "        self.id_to_string = {}\n",
        "        self.string_to_id = {}\n",
        "\n",
        "        # add the default pad token\n",
        "        self.id_to_string[0] = pad_token\n",
        "        self.string_to_id[pad_token] = 0\n",
        "\n",
        "        # add the default unknown token\n",
        "        self.id_to_string[1] = unk_token\n",
        "        self.string_to_id[unk_token] = 1\n",
        "\n",
        "        # add the default unknown token\n",
        "        self.id_to_string[2] = eos_token\n",
        "        self.string_to_id[eos_token] = 2\n",
        "\n",
        "        # add the default unknown token\n",
        "        self.id_to_string[3] = sos_token\n",
        "        self.string_to_id[sos_token] = 3\n",
        "\n",
        "        # shortcut access\n",
        "        self.pad_id = 0\n",
        "        self.unk_id = 1\n",
        "        self.eos_id = 2\n",
        "        self.sos_id = 3\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.id_to_string)\n",
        "\n",
        "    def add_new_word(self, string):\n",
        "        self.string_to_id[string] = len(self.string_to_id)\n",
        "        self.id_to_string[len(self.id_to_string)] = string\n",
        "\n",
        "    # Given a string, return ID\n",
        "    # if extend_vocab is True, add the new word\n",
        "    def get_idx(self, string, extend_vocab=False):\n",
        "        if string in self.string_to_id:\n",
        "            return self.string_to_id[string]\n",
        "        elif extend_vocab:  # add the new word\n",
        "            self.add_new_word(string)\n",
        "            return self.string_to_id[string]\n",
        "        else:\n",
        "            return self.unk_id\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, 'w') as f:\n",
        "            for word in self.string_to_id.keys():\n",
        "                f.write(word + '\\t' + str(self.string_to_id[word]) + '\\n')\n",
        "\n",
        "    def load(self, path):\n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                word, idx = line.split('\\t')\n",
        "                self.string_to_id[word] = int(idx)\n",
        "                self.id_to_string[int(idx)] = word\n",
        "\n",
        "\n",
        "# Read the raw txt file and generate a 1D pytorch tensor\n",
        "# containing the whole text mapped to sequence of token ID,\n",
        "# and a vocab file\n",
        "class ParallelTextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, src_file_path, trg_file_path, src_vocab=None,\n",
        "                 trg_vocab=None, extend_vocab=False, device='cpu'):\n",
        "        (self.data, self.src_vocab, self.trg_vocab,\n",
        "         self.src_max_seq_length, self.tgt_max_seq_length) = self.parallel_text_to_data(\n",
        "            src_file_path, trg_file_path, src_vocab, trg_vocab, extend_vocab, device)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def parallel_text_to_data(self, src_file, tgt_file, src_vocab=None, tgt_vocab=None,\n",
        "                          extend_vocab=False, device='cpu'):\n",
        "        # Convert paired src/tgt texts into torch.tensor data.\n",
        "        # All sequences are padded to the length of the longest sequence\n",
        "        # of the respective file.\n",
        "\n",
        "        assert os.path.exists(src_file)\n",
        "        assert os.path.exists(tgt_file)\n",
        "\n",
        "        if src_vocab is None:\n",
        "            src_vocab = Vocabulary()\n",
        "\n",
        "        if tgt_vocab is None:\n",
        "            tgt_vocab = Vocabulary()\n",
        "\n",
        "        data_list = []\n",
        "        # Check the max length, if needed construct vocab file.\n",
        "        src_max = 0\n",
        "        with open(src_file, 'r') as text:\n",
        "            for line in text:\n",
        "                tokens = list(line)\n",
        "                length = len(tokens)\n",
        "                if src_max < length:\n",
        "                    src_max = length\n",
        "\n",
        "        tgt_max = 0\n",
        "        with open(tgt_file, 'r') as text:\n",
        "            for line in text:\n",
        "                tokens = list(line)\n",
        "                length = len(tokens)\n",
        "                if tgt_max < length:\n",
        "                    tgt_max = length\n",
        "        tgt_max += 2  # add for begin/end tokens\n",
        "\n",
        "        src_pad_idx = src_vocab.pad_id\n",
        "        tgt_pad_idx = tgt_vocab.pad_id\n",
        "\n",
        "        tgt_eos_idx = tgt_vocab.eos_id\n",
        "        tgt_sos_idx = tgt_vocab.sos_id\n",
        "\n",
        "        # Construct data\n",
        "        src_list = []\n",
        "        print(f\"Loading source file from: {src_file}\")\n",
        "        with open(src_file, 'r') as text:\n",
        "            for line in tqdm(text):\n",
        "                seq = []\n",
        "                tokens = list(line)\n",
        "                for token in tokens:\n",
        "                    seq.append(src_vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "                var_len = len(seq)\n",
        "                var_seq = torch.tensor(seq, device=device, dtype=torch.int64)\n",
        "                # padding\n",
        "                new_seq = var_seq.data.new(src_max).fill_(src_pad_idx)\n",
        "                new_seq[:var_len] = var_seq\n",
        "                src_list.append(new_seq)\n",
        "\n",
        "        tgt_list = []\n",
        "        print(f\"Loading target file from: {tgt_file}\")\n",
        "        with open(tgt_file, 'r') as text:\n",
        "            for line in tqdm(text):\n",
        "                seq = []\n",
        "                tokens = list(line)\n",
        "                # append a start token\n",
        "                seq.append(tgt_sos_idx)\n",
        "                for token in tokens:\n",
        "                    seq.append(tgt_vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "                # append an end token\n",
        "                seq.append(tgt_eos_idx)\n",
        "\n",
        "                var_len = len(seq)\n",
        "                var_seq = torch.tensor(seq, device=device, dtype=torch.int64)\n",
        "\n",
        "                # padding\n",
        "                new_seq = var_seq.data.new(tgt_max).fill_(tgt_pad_idx)\n",
        "                new_seq[:var_len] = var_seq\n",
        "                tgt_list.append(new_seq)\n",
        "\n",
        "        # src_file and tgt_file are assumed to be aligned.\n",
        "        assert len(src_list) == len(tgt_list)\n",
        "        for i in range(len(src_list)):\n",
        "            data_list.append((src_list[i], tgt_list[i]))\n",
        "\n",
        "        print(\"Done.\")\n",
        "\n",
        "        return data_list, src_vocab, tgt_vocab, src_max, tgt_max\n"
      ],
      "metadata": {
        "id": "6qJUW9EI3Aq7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `DATASET_DIR` should be modified to the directory where you downloaded the dataset.\n",
        "DATASET_DIR = \"/content/drive/MyDrive/\"\n",
        "\n",
        "TRAIN_FILE_NAME = \"train\"\n",
        "VALID_FILE_NAME = \"interpolate\"\n",
        "\n",
        "INPUTS_FILE_ENDING = \".x\"\n",
        "TARGETS_FILE_ENDING = \".y\"\n",
        "\n",
        "TASK = \"numbers__place_value\"\n",
        "# TASK = \"comparison__sort\"\n",
        "# TASK = \"algebra__linear_1d\"\n",
        "\n",
        "# Adapt the paths!\n",
        "\n",
        "src_file_path = f\"{DATASET_DIR}/{TASK}/{TRAIN_FILE_NAME}{INPUTS_FILE_ENDING}\"\n",
        "trg_file_path = f\"{DATASET_DIR}/{TASK}/{TRAIN_FILE_NAME}{TARGETS_FILE_ENDING}\"\n",
        "\n",
        "train_set = ParallelTextDataset(src_file_path, trg_file_path, extend_vocab=True)\n",
        "\n",
        "# get the vocab\n",
        "src_vocab = train_set.src_vocab\n",
        "trg_vocab = train_set.trg_vocab\n",
        "\n",
        "src_file_path = f\"{DATASET_DIR}/{TASK}/{VALID_FILE_NAME}{INPUTS_FILE_ENDING}\"\n",
        "trg_file_path = f\"{DATASET_DIR}/{TASK}/{VALID_FILE_NAME}{TARGETS_FILE_ENDING}\"\n",
        "\n",
        "valid_set = ParallelTextDataset(\n",
        "    src_file_path, trg_file_path, src_vocab=src_vocab, trg_vocab=trg_vocab,\n",
        "    extend_vocab=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-MGTwMj3E7Q",
        "outputId": "e7306a06-9f49-40e9-c006-9d390d105017"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading source file from: /content/drive/MyDrive//numbers__place_value/train.x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999998it [01:04, 31101.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading target file from: /content/drive/MyDrive//numbers__place_value/train.y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999998it [00:30, 65311.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Loading source file from: /content/drive/MyDrive//numbers__place_value/interpolate.x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10000it [00:00, 33537.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading target file from: /content/drive/MyDrive//numbers__place_value/interpolate.y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10000it [00:00, 67443.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab.save('src_vocab.txt')\n",
        "trg_vocab.save('trg_vocab.txt')"
      ],
      "metadata": {
        "id": "kD21tJm6Fo1m"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab.string_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sktKFawTF5eH",
        "outputId": "edbcac0b-10a6-4ce7-98aa-df5c1f2271ad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<unk>': 1,\n",
              " '<eos>': 2,\n",
              " '<sos>': 3,\n",
              " 'W': 4,\n",
              " 'h': 5,\n",
              " 'a': 6,\n",
              " 't': 7,\n",
              " ' ': 8,\n",
              " 'i': 9,\n",
              " 's': 10,\n",
              " 'e': 11,\n",
              " 'u': 12,\n",
              " 'n': 13,\n",
              " 'd': 14,\n",
              " 'r': 15,\n",
              " 'g': 16,\n",
              " 'o': 17,\n",
              " 'f': 18,\n",
              " '3': 19,\n",
              " '1': 20,\n",
              " '2': 21,\n",
              " '5': 22,\n",
              " '?': 23,\n",
              " '\\n': 24,\n",
              " '8': 25,\n",
              " '9': 26,\n",
              " '6': 27,\n",
              " '7': 28,\n",
              " '4': 29,\n",
              " '0': 30,\n",
              " 'm': 31,\n",
              " 'l': 32,\n",
              " 'b': 33}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    dataset=train_set, batch_size=batch_size, shuffle=True, num_workers = 2, pin_memory = True)\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    dataset=valid_set, batch_size=batch_size, shuffle=False, num_workers = 2, pin_memory = True)"
      ],
      "metadata": {
        "id": "V4e-oSAo3STj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########\n",
        "# Taken from:\n",
        "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "# or also here:\n",
        "# https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
        "                             * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)\n",
        "        self.register_buffer('pe', pe)  # Will not be trained.\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "        assert x.size(0) < self.max_len, (\n",
        "            f\"Too long sequence length: increase `max_len` of pos encoding\")\n",
        "        # shape of x (len, B, dim)\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "XNkH_2Q03XQL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    # data_path for training data\n",
        "    parser.add_argument(\"data_path\", type=str, default=\"data\", help=\"data path\")\n",
        "\n",
        "    parser.add_argument('--use-memsizer', action='store_true', help='use memsizer in both encoder and decoder.')\n",
        "    parser.add_argument('--encoder-use-rfa', action='store_true', help='use memsizer in encoder.')\n",
        "    parser.add_argument('--decoder-use-rfa', action='store_true', help='use memsizer in decoder.')\n",
        "    parser.add_argument('--causal-proj-dim', type=int, default=4, help='the number of memory slots in causal attention.')\n",
        "    parser.add_argument('--cross-proj-dim', type=int, default=32, help='the number of memory slots in non-causal attention.')\n",
        "\n",
        "    parser.add_argument('--q-init-scale', type=float, default=8.0, help='init scale for \\Phi.')\n",
        "    parser.add_argument('--kv-init-scale', type=float, default=8.0, help='init scale for W_l and W_r.')\n",
        "    parser.add_argument(\n",
        "        \"-f\", action=\"store_true\", help=\"None\", default=False\n",
        "    )\n",
        "    parser.add_argument(\"--encoder_embed_dim\", default=512, type=int, help=\"embedding_dim\")\n",
        "    parser.add_argument(\"--encoder_ffn_embed_dim\", default=2048, type=int)\n",
        "    parser.add_argument(\"--encoder_layers\", default=4, type=int)\n",
        "    parser.add_argument(\"--encoder_attention_heads\", default=8, type=int, help=\"attention_heads\")\n",
        "\n",
        "    parser.add_argument(\"--decoder_embed_dim\", default=512, type=int)\n",
        "    parser.add_argument(\"--decoder_ffn_embed_dim\", default=2048, type=int)\n",
        "    parser.add_argument(\"--decoder_attention_heads\", default=8, type=int, help=\"attention heads\")\n",
        "    parser.add_argument(\"--decoder_layers\", default=4)\n",
        "\n",
        "    parser.add_argument(\"--dropout\", default=0.33, type=float, help=\"dropout\")\n",
        "    parser.add_argument('--attention-dropout', type=float, metavar='D', default=0,\n",
        "                            help='dropout probability for attention weights')\n",
        "    parser.add_argument('--encoder_normalize_before', action='store_false', default=True, help='apply layernorm before each encoder block')\n",
        "    parser.add_argument('--decoder_normalize_before', action='store_false', default=True, help='apply layernorm before each encoder block')\n",
        "    return parser.parse_args()\n"
      ],
      "metadata": {
        "id": "V0wB3Fup6v20"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Memsizer layer.\"\"\"\n",
        "\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from fairseq import utils\n",
        "from fairseq.modules import LayerNorm\n",
        "from fairseq.modules.fairseq_dropout import FairseqDropout\n",
        "from fairseq.modules.quant_noise import quant_noise\n",
        "from src.attention import CausalAttention, CrossAttention\n",
        "\n",
        "class MemsizerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, args\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = args.encoder_embed_dim\n",
        "        self.num_heads = args.encoder_attention_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "\n",
        "        self.dropout_module = FairseqDropout(\n",
        "            args.dropout, module_name=self.__class__.__name__\n",
        "        )\n",
        "        self.quant_noise = getattr(args, \"quant_noise_pq\", 0)\n",
        "        self.quant_noise_block_size = getattr(args, \"quant_noise_pq_block_size\", 8)\n",
        "\n",
        "        self.self_attn = self.build_self_attention(\n",
        "            self.embed_dim,\n",
        "            args\n",
        "        )\n",
        "\n",
        "        self.activation_fn = utils.get_activation_fn(\n",
        "            activation=str(args.activation_fn)\n",
        "            if getattr(args, \"activation_fn\", None) is not None\n",
        "            else \"relu\"\n",
        "        )\n",
        "        activation_dropout_p = getattr(args, \"activation_dropout\", 0) or 0\n",
        "        if activation_dropout_p == 0:\n",
        "            # for backwards compatibility with models that use args.relu_dropout\n",
        "            activation_dropout_p = getattr(args, \"relu_dropout\", 0) or 0\n",
        "        self.activation_dropout_module = FairseqDropout(\n",
        "            float(activation_dropout_p), module_name=self.__class__.__name__\n",
        "        )\n",
        "        self.normalize_before = args.encoder_normalize_before\n",
        "\n",
        "        # use layerNorm rather than FusedLayerNorm for exporting.\n",
        "        # char_inputs can be used to determint this.\n",
        "        # TODO  remove this once we update apex with the fix\n",
        "        export = getattr(args, \"char_inputs\", False)\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n",
        "\n",
        "        self.fc1 = self.build_fc1(\n",
        "            self.embed_dim,\n",
        "            args.encoder_ffn_embed_dim,\n",
        "            self.quant_noise,\n",
        "            self.quant_noise_block_size,\n",
        "        )\n",
        "        self.fc2 = self.build_fc2(\n",
        "            args.encoder_ffn_embed_dim,\n",
        "            self.embed_dim,\n",
        "            self.quant_noise,\n",
        "            self.quant_noise_block_size,\n",
        "        )\n",
        "\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n",
        "        self.need_attn = True\n",
        "\n",
        "        self.onnx_trace = False\n",
        "\n",
        "    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n",
        "        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n",
        "\n",
        "    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n",
        "        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n",
        "\n",
        "    def build_self_attention(\n",
        "        self, embed_dim, args\n",
        "    ):\n",
        "        return CrossAttention(\n",
        "            args=args,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=self.num_heads,\n",
        "            k_dim=args.cross_proj_dim,\n",
        "            q_noise=self.quant_noise,\n",
        "            qn_block_size=self.quant_noise_block_size,\n",
        "        )\n",
        "    def prepare_for_onnx_export_(self):\n",
        "        self.onnx_trace = True\n",
        "\n",
        "    def residual_connection(self, x, residual):\n",
        "        return residual + x\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_padding_mask,\n",
        "        attn_mask: Optional[Tensor] = None,\n",
        "    ):\n",
        "\n",
        "        assert attn_mask is None\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        x = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            key_padding_mask=encoder_padding_mask,\n",
        "            attn_mask=attn_mask\n",
        "        )\n",
        "\n",
        "        x = self.dropout_module(x)\n",
        "        x = self.residual_connection(x, residual)\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = self.activation_dropout_module(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout_module(x)\n",
        "        x = self.residual_connection(x, residual)\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return x\n",
        "\n",
        "class MemsizerDecoderLayer(nn.Module):\n",
        "    \"\"\"Decoder layer block.\n",
        "\n",
        "    In the original paper each operation (multi-head attention, encoder\n",
        "    attention or FFN) is postprocessed with: `dropout -> add residual ->\n",
        "    layernorm`. In the tensor2tensor code they suggest that learning is more\n",
        "    robust when preprocessing each layer with layernorm and postprocessing with:\n",
        "    `dropout -> add residual`. We default to the approach in the paper, but the\n",
        "    tensor2tensor approach can be enabled by setting\n",
        "    *args.decoder_normalize_before* to ``True``.\n",
        "\n",
        "    Args:\n",
        "        args (argparse.Namespace): parsed command-line arguments\n",
        "        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n",
        "            (default: False).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, args, no_encoder_attn=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = args.decoder_embed_dim\n",
        "        self.num_heads = args.decoder_attention_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "\n",
        "        self.dropout_module = FairseqDropout(\n",
        "            args.dropout, module_name=self.__class__.__name__\n",
        "        )\n",
        "        self.quant_noise = getattr(args, \"quant_noise_pq\", 0)\n",
        "        self.quant_noise_block_size = getattr(args, \"quant_noise_pq_block_size\", 8)\n",
        "\n",
        "        self.self_attn = self.build_self_attention(\n",
        "            self.embed_dim,\n",
        "            args\n",
        "        )\n",
        "\n",
        "        self.activation_fn = utils.get_activation_fn(\n",
        "            activation=str(args.activation_fn)\n",
        "            if getattr(args, \"activation_fn\", None) is not None\n",
        "            else \"relu\"\n",
        "        )\n",
        "        activation_dropout_p = getattr(args, \"activation_dropout\", 0) or 0\n",
        "        if activation_dropout_p == 0:\n",
        "            # for backwards compatibility with models that use args.relu_dropout\n",
        "            activation_dropout_p = getattr(args, \"relu_dropout\", 0) or 0\n",
        "        self.activation_dropout_module = FairseqDropout(\n",
        "            float(activation_dropout_p), module_name=self.__class__.__name__\n",
        "        )\n",
        "        self.normalize_before = args.decoder_normalize_before\n",
        "\n",
        "        # use layerNorm rather than FusedLayerNorm for exporting.\n",
        "        # char_inputs can be used to determint this.\n",
        "        # TODO  remove this once we update apex with the fix\n",
        "        export = getattr(args, \"char_inputs\", False)\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n",
        "\n",
        "        if no_encoder_attn:\n",
        "            self.encoder_attn = None\n",
        "            self.encoder_attn_layer_norm = None\n",
        "        else:\n",
        "            self.encoder_attn = self.build_encoder_attention(self.embed_dim, args)\n",
        "            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n",
        "\n",
        "\n",
        "\n",
        "        self.fc1 = self.build_fc1(\n",
        "            self.embed_dim,\n",
        "            args.decoder_ffn_embed_dim,\n",
        "            self.quant_noise,\n",
        "            self.quant_noise_block_size,\n",
        "        )\n",
        "        self.fc2 = self.build_fc2(\n",
        "            args.decoder_ffn_embed_dim,\n",
        "            self.embed_dim,\n",
        "            self.quant_noise,\n",
        "            self.quant_noise_block_size,\n",
        "        )\n",
        "\n",
        "\n",
        "        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n",
        "        self.need_attn = True\n",
        "\n",
        "        self.onnx_trace = False\n",
        "\n",
        "    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n",
        "        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n",
        "\n",
        "    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n",
        "        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n",
        "\n",
        "    def build_fc3(self, input_dim, output_dim, q_noise, qn_block_size):\n",
        "        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n",
        "\n",
        "    def build_self_attention(\n",
        "        self, embed_dim, args\n",
        "    ):\n",
        "        return CausalAttention(\n",
        "            args=args,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=self.num_heads,\n",
        "            k_dim=args.causal_proj_dim,\n",
        "            q_noise=self.quant_noise,\n",
        "            qn_block_size=self.quant_noise_block_size\n",
        "        )\n",
        "\n",
        "    def build_encoder_attention(self, embed_dim, args):\n",
        "        return CrossAttention(\n",
        "            args=args,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=self.num_heads,\n",
        "            k_dim=args.cross_proj_dim,\n",
        "            q_noise=self.quant_noise,\n",
        "            qn_block_size=self.quant_noise_block_size,\n",
        "        )\n",
        "\n",
        "    def prepare_for_onnx_export_(self):\n",
        "        self.onnx_trace = True\n",
        "\n",
        "    def residual_connection(self, x, residual):\n",
        "        return residual + x\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_out: Optional[Tensor] = None,\n",
        "        encoder_padding_mask: Optional[Tensor] = None,\n",
        "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "        prev_self_attn_state: Optional[List[Tensor]] = None,\n",
        "        prev_attn_state: Optional[List[Tensor]] = None,\n",
        "        self_attn_mask: Optional[Tensor] = None,\n",
        "        self_attn_padding_mask: Optional[Tensor] = None,\n",
        "        need_attn = False,\n",
        "        need_head_weights = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_state: s, z, random_matrices\n",
        "            encoder_padding_mask (ByteTensor, optional): binary\n",
        "                ByteTensor of shape `(batch, src_len)` where padding\n",
        "                elements are indicated by ``1``.\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        if prev_self_attn_state is not None:\n",
        "            prev_key, prev_value = prev_self_attn_state[:2]\n",
        "            saved_state: Dict[str, Optional[Tensor]] = {\n",
        "                \"prev_key\": prev_key,\n",
        "                \"prev_value\": prev_value,\n",
        "            }\n",
        "            if len(prev_self_attn_state) >= 3:\n",
        "                saved_state[\"prev_key_padding_mask\"] = prev_self_attn_state[2]\n",
        "            assert incremental_state is not None\n",
        "            self.self_attn._set_input_buffer(incremental_state, saved_state)\n",
        "        x = self.self_attn(\n",
        "            x=x,\n",
        "            key_padding_mask=self_attn_padding_mask,\n",
        "            attn_mask=self_attn_mask,\n",
        "            incremental_state=incremental_state\n",
        "        )\n",
        "        x = self.dropout_module(x)\n",
        "        x = self.residual_connection(x, residual)\n",
        "        if not self.normalize_before:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "        if self.encoder_attn is not None:\n",
        "            residual = x\n",
        "            if self.normalize_before:\n",
        "                x = self.encoder_attn_layer_norm(x)\n",
        "            if prev_attn_state is not None:\n",
        "                prev_key, prev_value = prev_attn_state[:2]\n",
        "                saved_state: Dict[str, Optional[Tensor]] = {\n",
        "                    \"prev_key\": prev_key,\n",
        "                    \"prev_value\": prev_value,\n",
        "                }\n",
        "                if len(prev_attn_state) >= 3:\n",
        "                    saved_state[\"prev_key_padding_mask\"] = prev_attn_state[2]\n",
        "                assert incremental_state is not None\n",
        "                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n",
        "\n",
        "            x = self.encoder_attn(\n",
        "                query=x,\n",
        "                key=encoder_out,\n",
        "                value=encoder_out,\n",
        "                key_padding_mask=encoder_padding_mask,\n",
        "                incremental_state=incremental_state,\n",
        "            )\n",
        "\n",
        "            x = self.dropout_module(x)\n",
        "            x = self.residual_connection(x, residual)\n",
        "            if not self.normalize_before:\n",
        "                x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        if self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = self.activation_dropout_module(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = self.dropout_module(x)\n",
        "        x = self.residual_connection(x, residual)\n",
        "        if not self.normalize_before:\n",
        "            x = self.final_layer_norm(x)\n",
        "        return x, None, None\n"
      ],
      "metadata": {
        "id": "Yq5HZNms5Lxb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()"
      ],
      "metadata": {
        "id": "puoHb02k9bRp"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = MemsizerEncoderLayer(args)\n",
        "decoder_layer = MemsizerDecoderLayer(args)"
      ],
      "metadata": {
        "id": "VxNz8UBB8PJS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_data_loader:\n",
        "  break"
      ],
      "metadata": {
        "id": "POHqv7iBnQKd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Memsizer(nn.Module):\n",
        "    def __init__(self, source_vocabulary_size, target_vocabulary_size,\n",
        "                 d_model=512, pad_id=0, encoder_layers=4, decoder_layers=4,\n",
        "                 dim_feedforward=2048, num_heads=8):\n",
        "        # all arguments are (int)\n",
        "        super().__init__()\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        self.embedding_src = nn.Embedding(source_vocabulary_size, d_model, padding_idx = pad_id)\n",
        "        self.embedding_tgt = nn.Embedding(target_vocabulary_size, d_model, padding_idx = pad_id)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        self.encoder = encoder_layer\n",
        "        self.decoder = decoder_layer\n",
        "        self.linear = nn.Linear(d_model, target_vocabulary_size)\n",
        "\n",
        "    def create_src_padding_mask(self, src):\n",
        "        # input src of shape ()\n",
        "        src_padding_mask = src.transpose(0, 1) == 0\n",
        "        return src_padding_mask\n",
        "\n",
        "    def create_tgt_padding_mask(self, tgt):\n",
        "        # input tgt of shape ()\n",
        "        tgt_padding_mask = tgt.transpose(0, 1) == 0\n",
        "        return tgt_padding_mask\n",
        "\n",
        "    # Implement me!\n",
        "    def greedy_decode(self, target, max_len, memory, memory_key_padding_mask):\n",
        "\n",
        "      ys = torch.ones(1, 1).fill_(3).type_as(target.data).to(DEVICE)\n",
        "      for i in range(max_len-1):\n",
        "          tgt_key_padding_mask = self.create_tgt_padding_mask(ys).to(DEVICE)\n",
        "          tgt_mask = (nn.Transformer.generate_square_subsequent_mask(ys.size(0))\n",
        "                      .type(torch.bool)).to(DEVICE)\n",
        "          tgt = self.embedding_tgt(ys)\n",
        "          tgt = self.pos_encoder(tgt)\n",
        "\n",
        "          out, _, _ = self.decoder(tgt, memory, self_attn_mask = tgt_mask, self_attn_padding_mask = tgt_key_padding_mask, encoder_padding_mask = memory_key_padding_mask)\n",
        "          # shift the target by one\n",
        "          out = out.transpose(0, 1)\n",
        "          prob = self.linear(out[:, -1])\n",
        "\n",
        "          _, next_word = torch.max(prob, dim=1)\n",
        "          next_word = next_word.item()\n",
        "\n",
        "          ys = torch.cat([ys, torch.ones(1, 1).type_as(target.data).fill_(next_word)], dim=0)\n",
        "\n",
        "          # stop crieria 1\n",
        "          if next_word == 2:\n",
        "              break\n",
        "\n",
        "      if ys.shape[0] < max_len:\n",
        "        new_seq = ys.data.new(max_len, 1).fill_(0)\n",
        "        new_seq[:ys.shape[0],:] = ys\n",
        "        ys = new_seq\n",
        "      return ys\n",
        "\n",
        "\n",
        "    def greedy_search(self, src, tgt):\n",
        "        src_key_padding_mask = self.create_src_padding_mask(src).to(DEVICE)\n",
        "        out = self.embedding_src(src)\n",
        "        out = self.pos_encoder(out)\n",
        "        encoder_out = self.encoder(out, encoder_padding_mask = src_key_padding_mask)\n",
        "        results = torch.ones(tgt.shape[1], tgt.shape[0]).type(torch.long).to(DEVICE)\n",
        "        for i in range(encoder_out.shape[1]):\n",
        "          memory = encoder_out[:,i,:].unsqueeze(dim = 1)\n",
        "          memory_key_padding_mask = src_key_padding_mask[i,:].unsqueeze(dim = 0)\n",
        "          result = self.greedy_decode(tgt[:,i], tgt.size()[0] + 1, memory, memory_key_padding_mask)\n",
        "          result = result.permute(1,0)\n",
        "          results[i,:] = result[:,1:]\n",
        "        return results\n",
        "\n",
        "    # Implement me!\n",
        "    def forward(self, src, tgt):\n",
        "        src_key_padding_mask = self.create_src_padding_mask(src).to(DEVICE)\n",
        "        tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).to(DEVICE)\n",
        "        memory_key_padding_mask = src_key_padding_mask\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            tgt.shape[0]).to(DEVICE)\n",
        "\n",
        "        tgt = self.embedding_tgt(tgt)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        out = self.embedding_src(src)\n",
        "        out = self.pos_encoder(out)\n",
        "\n",
        "        encoder_out = self.encoder(out, encoder_padding_mask = src_key_padding_mask)\n",
        "        decoder_out, _, _ = self.decoder(tgt, encoder_out, self_attn_mask = tgt_mask, self_attn_padding_mask = tgt_key_padding_mask, encoder_padding_mask = memory_key_padding_mask)\n",
        "\n",
        "        out = self.linear(decoder_out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "sX39o-dJ3X8d"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accu(y_true, y_pred):\n",
        "  # y tensor shape in (sequence_length, batch_size)\n",
        "  bol = (y_pred == y_true).all(dim=1)\n",
        "\n",
        "  correct = torch.sum(bol)\n",
        "\n",
        "  accu = correct / y_true.shape[0]\n",
        "\n",
        "  return accu"
      ],
      "metadata": {
        "id": "TB1EgmnpFpBZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(eval_model, valid_data_loader, criterion, trg_vocab):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    total_accu = 0.\n",
        "    ntokens = len(trg_vocab.id_to_string)\n",
        "\n",
        "    tb = len(valid_data_loader)\n",
        "    with torch.no_grad():\n",
        "        for batch_id, (X, y) in enumerate(valid_data_loader):\n",
        "            if batch_id == 200:\n",
        "              break\n",
        "            X = X.permute(1,0).to(DEVICE)\n",
        "\n",
        "            y_input = y[:,:-1]\n",
        "            y_expected = y[:,1:].to(DEVICE)\n",
        "            y_input = y_input.permute(1,0).to(DEVICE)\n",
        "            # get the output from the model\n",
        "            output = model(X, y_input)\n",
        "            output = output.permute(1, 2, 0)\n",
        "            total_loss += criterion(output, y_expected)\n",
        "            predicted = model.greedy_search(X, y_input).to(DEVICE)\n",
        "            total_accu += get_accu(y_expected, predicted)\n",
        "\n",
        "    loss = total_loss / tb\n",
        "    accu = total_accu / tb\n",
        "\n",
        "    print('Validation | loss {:5.2f} | accu {:8.2f}%'.format(loss, accu*100))\n",
        "\n",
        "    return loss, accu"
      ],
      "metadata": {
        "id": "uvzzlViVFs-7"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "zYeTCQLyQZ-b"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVFuDdE5Khll"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracy = []\n",
        "\n",
        "valid_accu = []\n",
        "valid_loss = []\n",
        "\n",
        "def train(log_interval, model, train_data_loader, optimizer, epoch, criterion, trg_vocab, k = 10, clip_rate = 0.5):\n",
        "  model.train()\n",
        "  total_loss = 0.\n",
        "  total_accu = 0.\n",
        "\n",
        "  val_accu = 0\n",
        "\n",
        "  N_count = 0\n",
        "  ntokens = len(trg_vocab.id_to_string)\n",
        "  tb = len(train_data_loader)\n",
        "  for batch_idx, (X, y) in tqdm(enumerate(train_data_loader)):\n",
        "    t = time.time()\n",
        "    X = X.permute(1,0).to(DEVICE)\n",
        "\n",
        "    y_input = y[:,:-1]\n",
        "    y_expected = y[:,1:].to(DEVICE)\n",
        "    y_input = y_input.permute(1,0).to(DEVICE)\n",
        "    # get the output from the model\n",
        "    output = model(X, y_input)\n",
        "\n",
        "    # calculate the loss\n",
        "    output = output.permute(1, 2, 0)\n",
        "\n",
        "\n",
        "    loss = criterion(output, y_expected)\n",
        "    loss.backward()\n",
        "    s = time.time()\n",
        "    s = time.time()\n",
        "\n",
        "    # gradient accumulation\n",
        "    if ((batch_idx+1) % k == 0 or (batch_idx+1 == tb)):\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip_rate)\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    \"\"\"accu = get_accu(y_expected, predicted)\n",
        "    total_accu += accu.item()\n",
        "    accuracy.append(accu.item())\"\"\"\n",
        "\n",
        "    if (batch_idx+1) % log_interval == 0 and batch_idx > 0:\n",
        "      cur_loss = total_loss / log_interval\n",
        "\n",
        "\n",
        "      print('Training | epoch {:3d} | {:5d}/{:5d} batch | loss {:5.2f} '.format(\n",
        "                    epoch, batch_idx + 1, tb, cur_loss))\n",
        "\n",
        "      # val_loss, val_accu = evaluate(model, valid_data_loader, criterion, trg_vocab) # evaluate using valid data set\n",
        "\n",
        "      # valid_accu.append(val_accu.item())\n",
        "      # valid_loss.append(val_loss.item())\n",
        "\n",
        "      total_loss = 0\n",
        "      total_accu = 0\n",
        "\n",
        "    elif (batch_idx+1) == tb:\n",
        "      cur_loss = total_loss / log_interval\n",
        "      # cur_accu = total_accu / log_interval\n",
        "\n",
        "\n",
        "      print('Training | epoch {:3d} done | {:5d}/{:5d} batch | loss {:5.2f}'.format(\n",
        "                    epoch, batch_idx + 1, tb, cur_loss))\n",
        "\n",
        "      val_loss, val_accu = evaluate(model, valid_data_loader, criterion, trg_vocab)\n",
        "\n",
        "      valid_accu.append(val_accu.item())\n",
        "      valid_loss.append(val_loss.item())\n",
        "\n",
        "      total_loss = 0\n",
        "      total_accu = 0\n",
        "\n",
        "\n",
        "    if val_accu > 0.9: # stop training if we get validation accuracy larger than 0.9\n",
        "      print(\"Training Done | Validation Accuracy: \",val_accu.item() * 100)\n",
        "      return"
      ],
      "metadata": {
        "id": "hGqT3MWFFwcE"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1 # The number of epochs\n",
        "best_model = None\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "src_vocab_size = src_vocab.__len__()\n",
        "trg_vocab_size = trg_vocab.__len__()\n",
        "model = Memsizer(src_vocab_size, trg_vocab_size).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(1000, model, train_data_loader, optimizer, epoch, criterion, trg_vocab, 10, clip_rate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SUrscvaFzlx",
        "outputId": "89a80e03-9cfe-493d-d89d-521cb4c0a1f5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1008it [00:26, 45.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  1000/31250 batch | loss  0.70 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2008it [00:48, 45.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  2000/31250 batch | loss  0.49 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3008it [01:09, 47.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  3000/31250 batch | loss  0.43 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4008it [01:30, 47.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  4000/31250 batch | loss  0.28 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5008it [01:52, 47.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  5000/31250 batch | loss  0.20 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6008it [02:13, 46.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  6000/31250 batch | loss  0.17 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "7008it [02:34, 47.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  7000/31250 batch | loss  0.16 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8008it [02:56, 47.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  8000/31250 batch | loss  0.16 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9008it [03:17, 46.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 |  9000/31250 batch | loss  0.15 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10008it [03:38, 46.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 10000/31250 batch | loss  0.14 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11008it [04:00, 46.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 11000/31250 batch | loss  0.14 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12008it [04:21, 47.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 12000/31250 batch | loss  0.13 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13008it [04:42, 47.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 13000/31250 batch | loss  0.11 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "14008it [05:04, 46.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 14000/31250 batch | loss  0.10 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15008it [05:25, 46.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 15000/31250 batch | loss  0.07 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "16008it [05:46, 47.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 16000/31250 batch | loss  0.06 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "17008it [06:08, 46.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 17000/31250 batch | loss  0.04 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18008it [06:29, 47.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 18000/31250 batch | loss  0.03 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19008it [06:50, 47.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 19000/31250 batch | loss  0.03 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20008it [07:12, 46.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 20000/31250 batch | loss  0.02 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "21008it [07:33, 47.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 21000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "22008it [07:54, 46.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 22000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23008it [08:15, 47.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 23000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "24008it [08:37, 47.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 24000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "25008it [08:58, 46.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 25000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "26008it [09:20, 47.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 26000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27008it [09:41, 46.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 27000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "28008it [10:02, 46.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 28000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "29008it [10:24, 47.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 29000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30008it [10:45, 46.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 30000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "31008it [11:06, 47.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 | 31000/31250 batch | loss  0.01 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "31248it [11:11, 40.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training | epoch   1 done | 31250/31250 batch | loss  0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "31249it [12:12, 42.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation | loss  0.00 | accu   100.00%\n",
            "Training Done | Validation Accuracy:  100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, valid_data_loader, criterion, trg_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8fazseiZfHO",
        "outputId": "c61bf4e0-fd37-4e07-9e56-11d1e7924941"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation | loss  0.00 | accu   100.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(7.2182e-05, device='cuda:0'), tensor(1., device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'memsizer.pth')"
      ],
      "metadata": {
        "id": "fTUhJV5AJiQw"
      },
      "execution_count": 61,
      "outputs": []
    }
  ]
}